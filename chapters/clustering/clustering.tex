\chapter{An expressive dissimilarity measure for relational clustering}\label{ch:clustering}


This chapter introduces a novel framework for relational data clustering.
The main contribution of this chapter is a new dissimilarity measure for relational objects based on the concept of a neighbourhood tree -- a compact summary of the neighbourhood of a relational object.
The proposed relational clustering framework will serve as a basis for the relational latent representation method introduced in the next chapter.
This chapter is based on the following publications:

\begin{quote}
	\bibentry{DBLP:conf/ecai/DumancicB16}
\end{quote}

\begin{quote}
	\bibentry{Dumancic2017a}
\end{quote}




\section{Introduction}
\label{sec:Intro}


%In relational learning, the data set contains instances with relationships between them.
%Standard learning methods typically assume data are i.i.d. (drawn independently from the same population) and ignore the information in these relationships.
%Relational learning methods do exploit that information, and this often results in better performance.
%Complex data, such as relational data, is ubiquitous to the modern world.
%Among the most notable examples are social networks, which typically consist of a network of people interacting with each other.
%Another example includes rich biological and chemical data that often contains many interaction between atoms, molecules or proteins.
%Finally, any data stored in the form of relational databases is essentially  relational data.
%Much research in relational learning focuses on supervised learning \cite{LucRLbook} or probabilistic graphical models \cite{GetoorSRL}.
%Clustering, however, has received less attention in the relational context.


Clustering is an under-specified learning task: there is no universal criterion for what makes a good clustering, it is inherently subjective.
This is known for i.i.d. data \cite{Estivill-Castro:2002}, and is even more true for relational data where the data set contains instances with relationships between them.
Different methods for relational clustering have very different biases, which are often left implicit; for instance, some methods represent the relational information as a graph (which means they assume a single binary relation) and assume that similarity refers to proximity in the graph, whereas other methods that take the relational database stance assume the similarity comes from the relationships objects participate in.
%, assuming typed objects that may participate in multiple, possibly non-binary, relationships.
Such strong implicit biases make use of a clustering algorithm difficult for a problem at hand, without a deep understanding of both the clustering method and the problem at hand.

\begin{figure}
  \centering
  \medskip
  \includegraphics[width=.9\textwidth]{bigpeople_new}
  \caption[Example relational dataset]{An illustration of a relational dataset containing people and organisations, and different clusters one might find in it. Instances - people (gray vertices) and organisations (white vertices), are represented by vertices, while relationships among them are represented with edges. The rectangles list an associated set of attributes for the corresponding vertex.}
  \label{fig:clustering:intro}
\end{figure}


In this chapter, we propose a very versatile framework for clustering relational data that makes the underlying biases transparent to a user.
It views a relational data set as a graph with typed vertices, typed edges, and attributes associated to the vertices.
This view is very similar to the viewpoint of relational databases or predicate logic.
The task we consider is clustering the vertices of one particular type.
What distinguishes our approach from other approaches is that the concept of (dis)similarity used here is very broad.
It can take into account attribute dissimilarity, dissimilarity of the relations an object participates in (including roles and multiplicity), dissimilarity of the neighbourhoods (in terms of attributes, relationships, or vertex identity), and inter-connectivity or graph proximity of the objects being compared.


Consider for example Figure \ref{fig:clustering:intro}.
This relational dataset describes people and organisations, and relationships between them (friendship, a persons’s role in the organisation, \ldots).
Persons and organisations are vertices in the graph shown there (shown as white/grey ellipses), the relationships between them are shown as edges, and their attributes are shown in dashed boxes.
Vertices can be clustered in very different ways:
\begin{enumerate}
    \item \texttt{Google} and \texttt{Microsoft} are similar because of their attributes, and could be clustered together for that reason
    \item \texttt{John}, \texttt{Rose} and \texttt{Han} form a densely interconnected cluster
    \item \texttt{Bob}, \texttt{Joe} and \texttt{Rose} share the property that they fulfil the role of supervisor
\end{enumerate}
Non-relational clustering systems will yield clusters such as the first one; they only look at the attributes of individuals.
Graph partitioning systems yield clusters of the second type.
Some relational clustering systems yield clusters of the third type, which are defined by local structural properties.
Most existing clustering systems have a very strong bias towards ``their'' type of clusters; a graph partitioning system, for instance, cannot possibly come up with the \{Google, Microsoft\} cluster, since this is not a connected component in the graph.
The new clustering approach we propose is able to find all types of clusters, and even clusters that can only be found by mixing the biases.








\section{Relational clustering over neighbourhood trees}
\label{sec:Approach}



\subsection{Hyper-graph Representation}

\begin{figure}
  \centering
  \medskip
  \includegraphics[width=.9\textwidth]{people}
  \caption[Representation paradigms of relational data]{\textbf{Representation paradigms of relational data.} \textit{Logical view} (Section \textit{a)}) represents the relational data set as a set of logical facts; the upper part represents the definition of each predicate, while the bottom part lists all facts. \textit{Database view} (section \textit{b)}) associates a single database table with each of the predicates in the logical view. In the \textit{graph view} (section \textit{c)}) each node (circle) represents an instance and an associated set of attributes (represented with a rectangle), while relations are represented by the edges.}
  \label{fig:clustering:people}
\end{figure}

Within relational learning, at least three different paradigms exist: inductive logic programming \cite{MuggletonR94}, which uses first-order logic representations; relational data mining \cite{DzeroskiB04}, which is set in the context of relational databases; and graph mining \cite{Cook2006}, where relational data are represented as graphs.
We illustrate the different types of representation in Figure \ref{fig:clustering:people}.
This example represents a set of people and organisations, and the relationships between them.
The relational database format \textit{(b)} is perhaps the most familiar to most people.
It has a table for each entity type (\texttt{Person, Organisation}) and for each relationship type between entities (\texttt{Works\_for, Friends}).
Each table contains multiple attributes, each of which can be an identifier for a particular entity (a \textit{key attribute}, e.g., \texttt{PName}), or a property of that entity (\texttt{Age,Gender,\ldots}).
The logic-based format \textit{(a)} is very similar; it consists of logical facts, where the predicate name corresponds to the table’s name and the arguments to the attribute values.
There is a one-to-one mapping between rows in a table and logical facts.
The logic based view allows for easy integration of background knowledge (in the form of first-order logic rules) with the data.
Finally, there is the attributed graph representation \textit{(c)}, where entities are nodes in the graph, binary relationships between them are edges, and nods and edges can have attributes.
This representation has the advantage that it makes the entities and their connectivity more explicit, and it naturally separates identifiers from real attributes (e.g., the \texttt{PName} attribute from the \texttt{Person} table is not listed as an attribute of \texttt{Person} nodes, because it only serves to uniquely identify a person, and in the graph representation the node itself performs that function).
A disadvantage is that edges in a graph can represent only binary relationships.


Though the different representations are largely equivalent, they provide different views on the data, which affects the clustering methods used.
For instance, a notion such as shortest path distance is much more natural in the graph view than in the logic-based view, while the fact that there are  different types of entities is more explicit in the database view (one table per type).
The distinction between entities and attribute values is explicit in the graph, but more implicit in the database view (key vs. non-key attributes) and absent in the logic view.


In this chapter, we will use a hyper-graph view that combines elements of all the above.
An oriented hyper-graph is a structure $H=(V,E)$ where $V$ is a set of vertices and $E$ a set of hyper-edges; a hyper-edge is an ordered multi-set whose elements are in $V$.
Directed graphs are a special case of oriented hyper-graphs where all hyper-edges have cardinality two.


A set of relational data is represented by a typed, labelled, oriented hyper-graph $(V,E,\tau,\lambda)$ with $V$ a set of vertices, $E$ a set of hyper-edges, and $\tau: (V \cup E) \rightarrow T_V \cup T_E$  a type function that assigns a type to each vertex and hyper-edge ($T_V$ is the set of vertex types, $T_E$ the set of hyper-edge types).
With each type $t \in T_V$ a set of attributes $A(t)$ is associated, and $\lambda$ maps each vertex $v$ to a vector of values, one value for each attribute in $A(\tau(v))$.
If $a \in A(\tau(v))$, we write $a(v)$ for the value of $a$ in $v$.


A relational database can be converted into the hyper-graph representation as follows.\footnote{For the logic-based representation, the conversion is analogous.}
For each table with only one key attribute (describing the entities identified by that key), a vertex type is introduced, whose attributes are the non-key attributes of the table.
Each row becomes one vertex, whose identifier is the key value and whose attribute values are the non-key attribute values in the row.
For each table with more than one key attribute (describing non-unary relationships among entities), a hyper-edge is introduced that contains the vertices corresponding to these entities in the order they occur in the table.
Our hyper-graph representation does not associate attributes with hyper-edges, only with vertices; hence, for non-unary relationships contain non-key attributes, a new vertex type corresponding to that hyper-edge type is introduced.


The clustering task we consider is the following: given a vertex type $t \in T_V$, partition the vertices of this type into clusters such that vertices in the same cluster tend to be similar, and vertices in different clusters dissimilar, for some subjective notion of similarity.
In practice, it is of course not possible to use a subjective notion; one uses a well-defined similarity function, which hopefully on average approximates well the subjective notion that the user has in mind.
The following section introduces \textit{neighbourhood trees}, a structure we use to compactly represent and describe a neighbourhood of a vertex.






\subsection{Neighbourhood tree}
\label{sec:NT}



\begin{algorithm}[t]
\SetAlgoLined
\SetKwData{NT}{NT}\SetKwData{Tovisit}{toVisit}\SetKwData{D}{$d'$}
\KwData{a hyper-graph $H = (V,E, \tau, \lambda)$ \\ \quad \quad \ \ a vertex of interest $v$ \\ \quad \quad \ \ a depth $d$}
\KwResult{a neighbourhood tree \NT}
\tcc{initialize neighbourhood tree}
 \NT = new neighbourhood tree\;
 \NT.\texttt{addRoot}(v)\;
 \NT.\texttt{labelVertex}(v) \tcc*{add type and attributes}

 \Tovisit = \{v\} \tcc*{vertices to process}
 \D = 1 \tcc*{depth indicator}

 \tcc{repeat until the pre-specified depth}
 \While{\D $\leq d$}{

    \ForEach{v' $\in$ \texttt{toVisit} } {

        \ForEach{outgoing edge $e$ of vertex v' } {
            \ForEach{vertex $v''$ in hyper-edge $e$} {
                \NT.\texttt{addVertex}($v''$)\;
                \NT.\texttt{addEdge}($v'$,$v''$)\;
                \NT.\texttt{labelVertex}($v''$) \tcc*{add type and attributes}
                \NT.\texttt{labelEdge}($v'$, $v''$) \tcc*{add edge type and position}
                \Tovisit = \Tovisit $\cup$ \{$v''$\}\;
            }
        }

        \Tovisit = \Tovisit$\setminus$ \{v',v\}
    }
    \D += 1\;
 }
 \caption{Neighbourhood tree construction}
 \label{algo:NT}
\end{algorithm}


A neighbourhood tree is a directed graph rooted at a vertex of interest, i.e. the vertex whose neighbourhood one wants to describe.
It is constructed simply by following the hyper-edges from the root vertex, as outlined in Algorithm \ref{algo:NT}.
The construction of the neighbourhood tree is parametrised with the pre-specified depth, a vertex of interest and the original hyper-graph.
Consider a vertex $v$.
For every hyper-edge $E$ in which $v$ participates (lines 7-13), add a directed edge from $v$ to each vertex $v' \in E$ (line 9).
Label each vertex with its type, and attach to it the corresponding attribute vector (line 10).
Label the edge with the hyper-edge type and the position of $v$ in the hyper-edge (recall that hyper-edges are ordered sets; line 11).
The vertices thus added are said to be at depth 1.
If there are multiple hyper-edges connecting vertices $v$ and $v'$, $v'$ is added each time it is encountered.
Repeat this procedure for each $v'$ on depth 1 (stored in variable \texttt{toVisit}).
The vertices thus added are at depth 2.
Continue this procedure up to some predefined depth $d$.
The root element is never added to the subsequent levels.
An example of a neighbourhood tree is given in Figure \ref{fig:clustering:ng}.


\begin{figure}
  \centering
  \medskip
  \includegraphics[width=.9\textwidth]{ntnew}
  \caption[An illustration of the neighbourhood tree]{\textbf{An illustration of the neighbourhood tree.} The domain contains two types of vertices - \textit{objects} (\texttt{A} and \texttt{B}) and \textit{elements} (\texttt{C, D} and \texttt{E}), and two fictitious relations: \texttt{R} and \texttt{F}. The vertices of type \textit{object} have an associated set of attributes. Section \textit{a)} contains the database view of the domain. Section \textit{b)} contains the corresponding hyper-graph view. Here, edges are represented with full lines, while hyper-edges are represented with dashed lines. Finally, section \textit{c)} contains the corresponding \textit{neighbourhood tree} for the vertex \texttt{A}.}
  \label{fig:clustering:ng}
\end{figure}




\section{Dissimilarity measure}
\label{sec:SimMes}





The main idea behind the proposed dissimilarity measure is to express a wide range of similarity biases that can emerge in relational data, as discussed and exemplified in Section \ref{sec:Intro}.
The proposed dissimilarity measure compares two vertices by comparing their neighbourhood trees.
It does this by comparing, for each level of the tree, the distribution of vertices, attribute values, and outgoing edge labels observed on that level.  Earlier work in relational learning has shown that distributions are a good way of summarising neighbourhoods \cite{Perlich:2006}.


The method for comparing distributions distinguishes between discrete and continuous domains.
For discrete domains (vertices, edge types, and discrete attributes), the distribution simply maps each value to its relative frequency in the observed multi-set of values, and the $\chi^2$-measure for comparing distributions \cite{ZhaoChiSquared} is used.
That is, given two multi-sets $A$ and $B$, their dissimilarity is defined as

\begin{equation}
d(A,B) = \sum_{x \in A \cup B} { (f_A(x)-f_B(x))^2  \over f_A(x) + f_B(x) }
\end{equation}

\noindent where $f_S(x)$ is the relative frequency of element $x$ in multi-set $S$ (e.g., for $A=\{a,b,b,c\}$, $f_A(a)=0.25$ and $f_A(b)=0.5$).


In the continuous case, we compare distributions by applying aggregate functions to the multi-set of values, and comparing these aggregates.
Given a set $\mathcal{A}$ of aggregate functions, the dissimilarity is defined as
\begin{equation}
    d(A,B) = \sum_{f \in \mathcal{A}} {f(A)-f(B) \over r }
\end{equation}
with $r$ a normalisation constant ($r = \max_M f(M) - \min_M f(M)$, with $M$ ranging over all multi-sets for this attribute observed in the entire set of neighbourhood trees).
In our implementation, we use the mean and standard deviation as aggregate functions.


The above methods for comparing distributions have been chosen for their simplicity and ease of implementation.  More sophisticated methods could be used.
The main point of this section, however, is {\em which} distributions are compared, not {\em how} they are compared.


We use the following notation.  For any neighbourhood tree $g$,
\begin{itemize}
\item[\textbullet] $V^l(g)$ is the multi-set of vertices at depth $l$ (the root having depth 0)
\item[\textbullet] $V^l_t(g)$ is the multi-set of vertices of type $t$ at depth $l$
\item[\textbullet] $B^l_{t,a}(g)$ is the multi-set of values of attribute $a$ observed among the nodes of type $t$ at depth $l$
\item[\textbullet] $E^l(g)$ is the multi-set of edge types between depth $l$ and $l+1$
\end{itemize}

E.g., for the neighbourhood tree in Figure~\ref{fig:clustering:ng}, we have
\begin{itemize}
    \item[\textbullet] $V^1(g) = $\{\texttt{B, C, D}\}
    \item[\textbullet] $V^1_{object}(g) = $\{\texttt{B}\}
    \item[\textbullet] $E^1(g) = $\{(\texttt{F,1}), (\texttt{R,1}), (\texttt{R,1})\}
    \item[\textbullet] $B^1_{object,Attr1}(g) = $\{\texttt{Y}\}
\end{itemize}






Let $\mathcal{N}$ be the set of all neighbourhood trees corresponding to the vertices of interest in a hyper-graph.
Let $norm(\cdot)$ be a \textit{normalisation operator}, defined as $$norm(f(g_1,g_2)) = \frac{f(g_1,g_2)}{\underset{g,g' \in \mathcal{N}}{\max} f(g,g')},$$ i.e., the normalisation operator divides the value of the function $f(g_1,g_2)$ of two neighbourhood trees $g_1$ and $g_2$ by the highest value of the function $f$ obtained amongst all pairs of neighbourhood trees.



Intuitively, the proposed method starts by comparing two vertices according to their attributes.
It then proceeds by comparing the properties of their neighbourhoods: which vertices are in there, which attributes they have and how are they interacting.
Finally, it looks at the proximity of vertices in a given hyper-graph.
Formally, the dissimilarity of two vertices $v$ and $v'$ is defined as the dissimilarity of their neighbourhood trees $g$ and $g'$, which is:
\begin{equation}
\label{eq:Sim}
\begin{split}
s(g,g’) = & w_1 \cdot \mathsf{ad}(g,g’) + w_2 \cdot \mathsf{nad}(g,g’) + w_3 \cdot \mathsf{cd}(g,g’) \\
     & + w_4 \cdot \mathsf{nd}(g,g’) + w_5 \cdot \mathsf{ed}(g,g’)
 \end{split}
\end{equation}

where $\sum_i w_i=1$ and
	\begin{itemize}
		\item \textit{attribute-wise dissimilarity} (Figure \ref{fig:ntsemantics}, red)
			{
			\begin{equation}
				\mathsf{ad}(g,g') = norm\left( \sum_{a \in A(\tau(v))} d(B^0_{t,a}(g), B^0_{t,a}(g')) \right)
			\end{equation}
			}

			measures the dissimilarity of the root elements $v$ and $v'$ according to their attribute-value pairs.

		\item \textit{neighbourhood attribute dissimilarity} (Figure \ref{fig:ntsemantics}, blue)

			{
			\begin{equation}
				\mathsf{nad}(g,g') = norm \left( \sum_{l=1}^{d} \sum_{t \in T_{V}} \sum_{a \in A(t)} d(B^l_{t,a}(g), B^l_{t,a}(g')) \right)
			\end{equation}
			}

			measures the dissimilarity of attribute-value pairs associated with the neighbouring vertices of the root elements, per level and vertex type.

		\item \textit{connection dissimilarity}
			{
			\begin{equation}
				\mathsf{cd}(g,g') = 1 - norm \left(| \{ v \in V^0(g) | v \in V^1(g') \} | \right)
			\end{equation}
			}

			reflects the number of edges of different type that exist between the two root elements.

		\item \textit{neighbourhood dissimilarity} (Figure \ref{fig:ntsemantics}, green)
			{
			\begin{equation}
				\mathsf{nd}(g,g') = norm \left(\sum_{l=1}^{\#levels} \sum_{t \in T_{v}} d( V^l_{t}(g), V^l_{t}(g')) \right)
			\end{equation}
			}

			measures the dissimilarity of two root elements according to the vertex identities in their neighbourhoods, per level and vertex type.

		\item \textit{edge distribution dissimilarity} (Figure \ref{fig:ntsemantics}, purple):
			\begin{equation}
				\mathsf{ed}(g,g’) = norm \left(\sum_{l=1}^{\#levels} d(E^l(g), E^l(g’)) \right)\,
			\end{equation}
			measures the dissimilarity over edge types present in the neighbourhood trees, per  level.
	\end{itemize}

	\begin{figure}
		\centering
		\includegraphics[width=.9\linewidth]{ntSemantics}
		\caption{Semantic decomposition of neighbourhood tree\label{fig:ntsemantics}}
	\end{figure}

\newglossaryentry{recent}{name={ReCeNT},description={Relational Clustering over Neighbourhood Trees}}
Each component is normalised to the scale of 0-1 by the highest value obtained amongst all pair of vertices, ensuring that the influence of each factor is proportional to its weight.
The weights $w_{1-5}$ in Equation \ref{eq:Sim} allow one to formulate a bias through the similarity measure.
For the remainder of the text, we will term our approach as \gls{recent} (for Relational Clustering using Neighbourhood Trees).
The benefits and downsides of this formulation are discussed and contrasted to the existing approaches in Sections \ref{sec:RelClust} and \ref{sec:ResultsSub}.


This formulation is somewhat similar to the \textit{multi-view clustering} \cite{Bickel:2004}, with each of the components forming a different view on data.
However, there is one important fundamental difference: multi-view clustering methods want to find clusters that are good in each view separately, whereas our components do not represent different views on the data, but different potential biases, which jointly contribute to the similarity measure.


\section{Positioning in the literature}


\subsection{Hyper-graph representation}

Two interpretations of the hyper-graph view of relational data exist in literature.
The one we incorporate here, where domain objects form vertices in a hyper-graph with associated attributes, and their relationships form hyper-edges, was first introduced by \cite{Richards:92AAb}.
An alternative view, where logical facts form vertices, is presented by \cite{Ong2005}.
Such representations were later utilised to learn the formulas of relational models by \textit{relational path-finding} \cite{kok2010motifs,Richards:92AAb,Ong2005,Lovasz1996}.



The neighbourhood tree introduced in Section \ref{sec:NT} can be seen as summary of all paths in a hyper-graph originating at a certain vertex.
Though neighbourhood trees and relational path-finding rely on a hyper-graph view, the tasks they solve are conceptually different.
Whereas the goal of the neighbourhood tree is to compactly represent a neighbourhood of a vertex by summarising all the paths originating at the vertex, the goal of relational path-finding is to identify a small set of important paths that appear often in a hyper-graph.
Additionally, a practical difference is the distinction between hyper-edges and attributes - a neighbourhood tree is constructed by following only the hyper-edges, while the mentioned work either treats attributes as unary hyper-edges or requires a declarative bias from the user.


\subsection{Related tasks}

Two problems related to the one we consider here are graph and tree partitioning \cite{bader2012dimacs}.
Graph partitioning focuses on partitioning the original graph into a set of smaller graphs such that certain properties are satisfied.
Though such partitions can be seen as clusters of vertices, the clusters are limited to vertices that are connected to each other.
Thus, the problem we consider here is strictly more general, and does not put any restriction of that kind on the cluster memberships; the (dis)similarity of vertices can originate in any of the (dis)similarity sources we consider, most of which cannot be expressed within a graph partitioning problem.



A number of tree comparison techniques \cite{Bille:2005} exists in the literature.
These approaches consider only the identity of vertices as a source of similarity, while ignoring the attributes and types of both vertices and hyper-edges.
Thus, they are not well suited for the comparison of neighbourhood trees.



\subsection{Relational clustering}
\label{sec:RelClust}

The relational learning community, as well as the graph kernel community have previously shown interest in clustering relational (or structured) data.
Existing similarity measures within the relational learning community can be coarsely divided into two groups.


\newglossaryentry{hsag}{name={HSAG},description={Hybrid similarity on Annotated graphs}}
\newglossaryentry{hs}{name={HS},description={Hybrid similarity}}
The first group consists of similarity measures defined over an attributed graph model \cite{Pfeiffer2014}, with examples in  \textit{Hybrid similarity (\gls{hs})} \cite{Neville03clusteringrelational} and \textit{Hybrid similarity on Annotated Graphs (\gls{hsag})} \cite{WitsenburgB11a}.
Both approaches focus on attribute-based similarity of vertices where \gls{hs} compares the attributes of all connected vertices, and \gls{hsag}'s similarity measure compares attributes of the vertices themselves and attributes of their neighbouring vertices.
The main limitations of these approaches are that they ignore the existence of vertex and edge types, and impose a very strict bias towards attributes of vertices.
In comparison to the presented approach, HS defines dissimilarity as the $\mathsf{ad}$ component if there is an edge between two vertices, and $\infty$ otherwise.
\gls{hsag} defines the dissimilarity as a linear combination of the $\mathsf{ad}$ and $\mathsf{nad}$ components for each pair of vertices.



\newglossaryentry{cc}{name={CC},description={Relational Conceptual Clustering}}
\newglossaryentry{ribl}{name={RIBL},description={Relational Instance-Based Learning}}
In contrast to the first group which employs a graph view, the second group of methods employs a predicate logic view,
The two most prominent approaches are \textit{Conceptual clustering of Multi-relational Data} (\gls{cc}) \cite{Fonseca2012} and \textit{Relational instance-based learning} (\gls{ribl}) (\cite{RIBL96}, \cite{Kirsten98relationaldistance-based}).
\gls{cc} firstly describes each example (corresponding to a vertex in our problem) with a set of logical clauses that can be generated by a \textit{bottom clause saturation} \cite{CamachoFRC07}.
The obtained clauses are considered as features, and their similarity is measured by  the \textit{Tanimoto similarity} - a measure of overlap between sets.
In that sense, it is similar to using the $\mathsf{ad}$ and $\mathsf{ed}$ components for generating clauses.
Note that this approach does not differentiate between relations (or interactions) and attributes, does not consider distributions of any kind, and does not have a sense of depth of a neighbourhood.
Finally, \gls{ribl} follows an intuition that the similarity  of two objects depends on the similarity of their attributes' values and the similarity of the objects related to them.
To that extent, it first constructs a \textit{context descriptor} - a set of objects related to the object of interest, similarly to the neighbourhood trees.
Comparing two object now involves comparing their features and computing the similarity of the set of objects they are linked to.
That requires matching each object of one set to the most similar object in the other set, which is an expensive operation (proportional to the product of the set sizes).
In contrast, the $\chi^2$ distance is linear in the size of the multi-set.
Further, the $\chi^2$ distance takes the multiplicity of elements into account (it essentially compares distributions), which the \gls{ribl} approach does not.


\newglossaryentry{wlst}{name={WLST},description={Weisfeiler-Lehman subtree kernel}}
Within the graph kernel community, two prominent groups exist:  \textit{Weisfeiler-Lehman graph kernels (WL)} \cite{Shervashidze2011,shervashidze09fastsubtree,FrasconiCRG14,haussler99convolution,ICPR2014BaiRH}  and \textit{random walk based kernels}~\cite{WachmanK07,Lovasz1996}.
A common feature of these approaches is that they rely on global structural properties of graphs.
The Weisfeiler-Lehman Graph Kernels are a family of graph kernels developed upon the \textit{Weisfeiler-Lehman isomorphism test}.
The key idea of the WL isomorphism test is to extend the set of vertex attributes by the attributes of the set of neighbouring vertices, and compress the augmented attribute set into new set of attributes.
There each new attribute of a vertex corresponds to a subtree rooted from the vertex, similarly to the neighbourhood trees.
Shervashidze and Borgwardt have introduced a fast WL subtree kernel (\gls{wlst}) \cite{shervashidze09fastsubtree} for un-directed graphs by performing the WL isomorphism test to update the vertex labels, followed by counting the number of matched vertex labels.
The difference between our approach and WL kernel family is subtle but important: WL graph kernels extend the set of attributes by identifying isomorphic sub-trees present in (sub)graphs.
This is reflected in the bias they impose, that is, the similarity comes from the structure of a graph (in our case, a neighbourhood tree).


\newglossaryentry{rkoh}{name={RKOH},description={A rooted kernel for ordered hyper-graphs}}
\textit{A Rooted Kernel for Ordered hyper-graph (RKOH)} \cite{WachmanK07} is an instance of random walk kernels successfully applied in relational learning tasks.
These approaches estimate the similarity of two (hyper)graphs by comparing the walks one can obtain by traversing the hyper-graph.
\gls{rkoh} defines a similarity measure that compares two hyper-graphs by comparing the paths originating at every edge of both hyper-graphs, instead of the paths originating at the root of the hyper-graph.
\gls{rkoh} does not differentiate between attributes and hyper-edges, but treats everything as an hyper-edge instead (an attribute can be seen as an unary edge).


\begin{table}
    \centering

    \caption[Aspects of similarity considered by various relational clustering approaches]{\textbf{Aspects of similarity considered by various relational clustering approaches.} $\checkmark$ denotes full consideration, $\backsimeq$ partial and $\times$ no consideration at all.  }
    \label{tab:Props}
    \begin{tabular}{@{}lrrrrr@{}}
        \toprule
        \textbf{Similarity}  & {\footnotesize\textbf{Attributes}} & \makecell{\footnotesize\textbf{Neighbour.} \\ \footnotesize\textbf{attributes}} & \makecell{\footnotesize\textbf{Neighbour.} \\ \footnotesize\textbf{identities}}  & {\footnotesize\textbf{Proximity}} & \makecell{\footnotesize\textbf{Structural} \\ \footnotesize\textbf{properties}}    \\
        \midrule
         \gls{recent}                & $\checkmark$        & $\checkmark$                      & $\checkmark$                        & $\checkmark$       & $\checkmark$ \\

         \gls{hs}                  & $\checkmark$        & $\times$                          & $\times$                            & $\times$           & $\times$   \\

         \gls{hsag}               & $\checkmark$        & $\checkmark$                      & $\times$                            & $\times$           & $\times$   \\

         \gls{ribl}               & $\checkmark$        & $\checkmark$                      & $\checkmark$                        & $\times$           & $\times$  \\

         \gls{cc}                  & $\backsimeq$        & $\backsimeq$                      & $\times$                            & $\times$           & $\backsimeq$   \\

         \gls{rkoh}                & $\times$            & $\backsimeq$                      & $\times$                            & $\times$           & $\checkmark$ \\

         \gls{wlst}                & $\times$            & $\backsimeq$                      & $\times$                            & $\times$           & $\checkmark$ \\
         \bottomrule
    \end{tabular}

\end{table}


Table \ref{tab:Props} summarises different aspects of similarity considered by the above mentioned approaches.
The interpretations of similarity are divided into five sources of similarity.
The first two categories concern attributes: attributes of the vertices themselves and their neighbouring vertices.
The following two categories concern identities of vertices in the neighbourhood of a vertex of interest.
They concern sub-graphs (identity of vertices in the neighbourhood) centred at a vertex, and proximity of two vertices.
The final category concerns the structural properties of sub-graphs in the neighbourhood of a vertex defined by the neighbourhood tree.


\subsection{Complexity analysis}

Though scalability is not the focus of this work, here we show that the proposed approach is as scalable as the state-of-the-art kernel approaches, and substantially less complex than the majority of the above-mentioned approaches that use both attribute and link structure.
For the sake of clarity of comparison, assume a homogeneous graph with only one vertex type and one edge type.
Let $N$ be the number of vertices in a hyper-graph, $L$ be the total number of hyper-edges, and $d$ be the depth of a neighbourhood representation structure, where applicable.
Let, as well, $A$ be the number of attributes in a data set.
Additionally, assume that all vertices participate in the same number of hyper-edges, which we will refer to as $E$.
We will refer to the length of clause in CC and path in RKOH as $l$.



\begin{table}
    \centering
    \caption{Complexities of the various methods for relational clustering}
    \label{tab:complexities}
    \begin{tabular}{@{}lr@{}}
        \toprule
        \textbf{Approach} & \textbf{Complexity} \\
        \midrule
        \gls{hs}    & $O\left (  LA \right )$       \\

         \gls{hsag}  & $O \left ( N^2EA \right)$     \\

        \textbf{ReCeNT}  & $O \left ( N^2  E^d  \right )$ \\

         \gls{wlst}  & $O \left( N^2 E^d \right ) $  \\

        \gls{cc}    & $O \left( N^2   {{E + A}\choose{l}} \right)$  \\

        \gls{ribl}  & $O \left(  N^2 \prod_{k=1}^{d}(E + A)^{2k} \right) $  \\

         \gls{rkoh}  & $O \left(  N^2 \left( E + A \right)^{2d + 2l} \right)$ \\
        \bottomrule
    \end{tabular}

\end{table}



To compare any two vertices, \gls{recent} requires one to compute the dissimilarity of the multi-sets representing the vertices, proportional to $O(d\times A + \sum_{k=1}^d E^k) = O\left (N^2 E^d \right )$.
Table \ref{tab:complexities} summarises the complexities of the discussed approaches.
In summary, the approaches can be grouped into three categories.
The first category contains \gls{hs} and \gls{hsag}; these are substantially less complex than the rest, but focus only on the attribute similarities.
The second category contains \gls{ribl} and \gls{rkoh}, which are substantially more complex than the rest.
Both of these approaches use both attribute and edge information, but in a computationally very expensive way.
The last category contains \gls{recent}, \gls{wlst} and \gls{cc}; these lie in between. They utilise both attribute and edge information, but in a way that is much more efficient than \gls{ribl} and \gls{rkoh}.




The complexity of \gls{recent} benefits mostly from two design choices: \textit{differentiation of attributes and hyper-edges}, and \textit{decomposition of neighbourhood elements into multi-sets}.
By distinguishing hyper-edges from attributes, \gls{recent} focuses on identifying sparse neighbourhoods.
Decomposition of neighbourhoods into multi-sets allows \gls{recent} to compute the similarity linearly in the size of a multi-set.
The parameter that \gls{recent} is the most sensitive to is the depth of the neighbourhood tree, which is the case with the state-of-the-art kernel approaches as well.
However, the main underlying assumption behind \gls{recent} is that important information is contained in small local neighbourhoods, and \gls{recent} is designed to utilise such information.


\section{Evaluation}
\label{sec:Evaluation}



\subsection{Data sets}



We evaluate our approach on five data sets for relational clustering with different characteristics and domains.
The chosen data sets are commonly used within the (statistical) relational learning community, and they expose different biases.
The characterisation of data sets, summarised in Table~\ref{tab:Data}, include the total number of vertices in a hyper-graph, the number of vertices of interest, the total number of attributes, the number of attributes associated with vertices of interest, the number of hyper-edges as well as the number of different hyper-edge types.
The data sets range from having a small number of vertices, attributes and hyper-edges (UW-CSE, IMDB), to a considerably large number of vertices, attributes or hyper-edges (Mutagenesis, WebKB, TerroristAttack).
All the chosen data sets are originally classification data sets, which allows us to evaluate our approach with respect to how well it extracts the classes present in the data set.




\begin{table}
\captionsetup{justification=justified}
\begin{center}
\caption[Characteristics of the datasets used in the relational clustering experiments]{Characteristics of the data sets used in experiments. The characteristics include the total number of vertices, the number of vertices of interest, the total number of attributes, the number of attributes associated with vertices of interest, the number of hyper-edges as well as the number of different hyper-edge types.}
\label{tab:Data}
\begin{tabular}[t]{@{}lrrrrr@{}}


		\toprule
						  & \multicolumn{5}{c}{\textbf{Dataset}} \\
						  \cmidrule{2-6}
		\textbf{Property} & IMDB   & UW-CSE & Muta & WebKB & Terror \\
		\midrule
		vertices & 298  & 734 & 6124 & 3880  & 1293\\

		target vertices & 268 & 272  & 230 & 920  & 1293 \\

		vertex types & 3  & 4 & 2  & 2  & 1 \\

		attributes & 3   & 7 & 7 & 1207  & 106 \\

		target attributes & 3  & 3 & 4 & 763  & 106\\

		hyper-edges & 715  & 1834 & 30804 & 5779  & 3743\\

		hyper-edge types  & 3   & 6 & 7 & 4 & 2 \\
		\bottomrule


\end{tabular}
\end{center}

\end{table}


The IMDB\footnote{Available at http://alchemy.cs.washington.edu/data/imdb} data set is a small snapshot of the Internet Movie Database.
It describes a set of movies with people acting in or directing them.
The goal is to differentiate people into two groups: \textit{actors} and \textit{directors}.
The UW-CSE\footnote{Available at http://alchemy.cs.washington.edu/data/uw-cse/} data set describes the interactions of employees at the University of Washington and their roles, publications and the courses they teach.
The task is to identify two clusters of people: \textit{students} and \textit{professors}.
The Mutagenesis\footnote{Available at http://www.cs.ox.ac.uk/activities/machlearn/mutagenesis.html} data set, as described is Section~\ref{sec:Intro}, describes chemical compounds and atoms they consist of.
Both compounds and atoms are described with a set of attributes describing their chemical properties.
The task is to identify two clusters of compounds: \textit{mutagenic} and \textit{not mutagenic}.
The WebKB\footnote{Available at http://alchemy.cs.washington.edu/data/webkb/} data set consists of pages and links collected from the Cornell University's webpage.
Both pages and links are associated with a set of words appearing on a page or in the anchor text of a link.
The pages are classified into seven groups according to their role, such as \textit{personal}, \textit{departmental} or \textit{project} page.
The final data set, termed Terrorists\footnote{Available at http://linqs.umiacs.umd.edu/projects//projects/lbc/} \cite{sen:aimag08}, describes terrorist attacks each assigned one of 6 labels indicating the type of the attack.
Each attack is described by a total of 106 distinct features, and two relations indicating whether two attacks were performed by the same organisation or at the same location.



\subsection{Experimental setup}

In the remainder of this section, we evaluate our approach.
We focus on answering the following questions:

\begin{itemize}
    \item[\textbf{(Q1)}]\textit{How well does \gls{recent} perform on the relational clustering tasks compared to existing similarity measures?}

    \item[\textbf{(Q2)}] \textit{How relevant is each of the components?}  We perform clustering using our similarity measure and setting the parameters as $w_i = 1, w_{j, j \not=i}=0$.

    \item[\textbf{(Q3)}] \textit{To which extent can the parameters of the proposed similarity measure  be learnt from data in an unsupervised manner?}

    \item[\textbf{(Q4)}] \textit{How well does \gls{recent} perform compared to existing similarity measures in a supervised setting?}

    \item[\textbf{(Q5)}] \textit{How do the runtimes for \gls{recent} compare to the competitors?}
\end{itemize}








In each experiment, we have used the aforementioned (dis)similarity measures in conjunction with spectral \cite{Spectral},  and hierarchical \cite{Agglomerative} clustering algorithms.
We have intentionally chosen two clustering approaches which assume different biases, to be able to see how each similarity measure is affected by assumptions clustering algorithms make.
We have altered the depth on neighbourhood trees between 1 and 2 wherever it was possible, and report both results.




We evaluate each approach using the following validation method: we set the number of clusters to be equal to the true number of clusters in each data set, and evaluate the obtained clustering with regards to how well it matches the known clustering given by the labels.
Each obtained clustering is then evaluated using the \textit{adjusted Rand index} (ARI)  \cite{Rand71,MoreyARI}.
The ARI measures the similarity between two clusterings, in this case between the obtained clustering and the provided labels.
The ARI score ranges between -1 and 1, where a score closer to 1 corresponds to higher similarity between two clusterings, and hence better performance, while 0 is the chance level.
For each data set, and each similarity measure, we report the ARI score they achieve.
Additionally, we have set a timeout to 24 hours and do not report results for an approach that takes more time to compute.





To achieve a fair time comparison, we implemented all similarity measures (\gls{hs}, \gls{hsag}, \gls{ribl}, \gls{cc}, as well as \gls{rkoh}) in \texttt{Scala} and optimised them in the same way, by caching all the intermediate results that can be re-used.
We have used the clustering algorithms implemented in Python's \texttt{scikit-learn} package \cite{scikit-learn}.
The hierarchy obtained by hierarchical clustering was cut when it has reached the pre-specified number of clusters.
In the first experiment, the weights $w_{1-5}$ were not tuned, and were set to 0.2.
We have used mean and standard deviation as aggregates for continuous attributes.






\subsection{Results}
\label{sec:ResultsSub}



\subsubsection{\textbf{(Q1) Comparison to the existing methods}}

We compare \gls{recent} to a pure attribute based approach (termed Baseline), \gls{hs} \cite{Neville03clusteringrelational}, \gls{hsag} \cite{WitsenburgB11a}, \gls{cc} \cite{Fonseca2012}, \gls{ribl} \cite{RIBL96}, as well as \gls{wlst}~\cite{shervashidze09fastsubtree}, Linear kernel between vertex histograms (V), Linear kernel between vertex-edge histograms (VE) provided with \cite{NIPS2015_5688}, and \gls{rkoh}~\cite{WachmanK07}.
The subscript in \gls{recent}, \gls{hsag}, \gls{ribl} and kernel approaches  denotes the depth of the neighbourhood tree (or other supporting structure).
The subscript in \gls{cc} denotes the length of the clauses.
The second subscript in \gls{wlst} and \gls{rkoh} indicates their parameters: with \gls{wlst} it is the $h$ parameter indicating the number of iterations, whereas with \gls{rkoh} it indicates the length of the walk.




\begin{table}[t]
%\captionsetup{justification=centerlast}
\centering
\footnotesize
\caption[Performance of relational clustering approaches]{Performance of all approaches. For each similarity measure, the ARI achieved when the true number of clusters was used. The results are shown for both hierarchical and spectral clustering, while the depth of the approaches is indicated by the subscript. The last column counts the number of wins per algorithm, where ''win'' means achieving the highest ARI on a data set.}
\label{tab:ClustResults}
\resizebox{\linewidth}{!}{%
\begin{tabular}[t]{@{}lrrrrrrrrrrr@{}}
	\toprule
	\textbf{Similarity} & \multicolumn{2}{r}{\textbf{Muta}} & \multicolumn{2}{r}{\textbf{UWCSE}} & \multicolumn{2}{r}{\textbf{WebKB}} & \multicolumn{2}{r}{\textbf{Terror}}  & \multicolumn{2}{r}{\textbf{IMDB}} & \textbf{W} \\
	\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
	                     &  H       & S              & H      & S                 & H      & S         & H    & S           & H   & S &  \\
	\midrule
	\textbf{Baseline}     &  -0.02 & -0.03		     &   0.25 	&  0.2         	  &  0.00  &  0.25     & 0.00 & 0.17 		& 0.05 & 0.05 & 0  \\

	\textbf{HS}          &  -	 & -		     	 &   0.01 	&  0.06           &  0.0 &  0.10      	& 0.01 & -0.01		& 0.00 & 0.00 & 0 \\

	\textbf{CC$_2$}      &  0.00 &  0.01	     	 &  0.1	 &  0.82       	  	   & 0.00 & 0.04       & 0.01 & 0.01 		& 0.1 & 0.1  & 0\\

	\textbf{CC$_4$}      &  0.00 &  0.01	     	 &  0.00 & 0.92 	   & 0.00 & 0.04       			& 0.01 & 0.01  		& 0.1 & 0.1 & 0 \\
	\midrule
	\textbf{ReCeNT$_1$}    &  \textbf{0.32} & \textbf{0.35}     &\textbf{0.97} 	&  \textbf{0.98}   &  \textbf{0.04  } &  \textbf{0.57} & 0.00 & \textbf{0.26} & 0.62 & \textbf{1.0}  & \textbf{8}\\

	\textbf{RIBL$_1$}    &  0.22 & 0.26 		     &   0.89       	& 0.68      &  0.0           	 &  0.1   & - & -  & 0.35 &  0.38 & 0   \\

	\textbf{HSAG$_1$}    &  -0.01 & 0.06           &   0.1        	&  0.0               &  0.01       	 &  0.05    & 0.00 & 0.24 & 0.04 &  -0.05 & 0    \\

	\textbf{WLST$_{1,5}$}    & 0.00 & 0.02        &    -0.01    	&   0.33     &    0.00 	 & 0.33 & \textbf{0.27} & 0.07  & -0.01 & 0.66 & 1      \\

	\textbf{WLST$_{1,10}$}    & 0.00 & 0.02      &   -0.01    & 0.33        &   0.00  & 0.32  & \textbf{0.27} & 0.11 & -0.01 & 0.31 & 1       \\

	\textbf{V$_1$}    & 0.00  & 0.03         &   -0.01       	&   0.19      &   0.00    	 &  0.00   & 0.00 & 0.00  & 0.00 & 0.00 & 0     \\

	\textbf{VE$_1$}    & 0.00 & 0.03         &     0.01   	& 0.36         &  0.00 	 & 0.00   &0.00  & 0.00 & \textbf{1.0}  &  \textbf{1.0} & 2     \\

	\textbf{RKOH$_{1,2}$}    & 0.1 & 0.1         &  0.2      	&   0.2       &  - 	 & -   & -  & - 			& 0.83  &  0.83 & 0    \\

	\textbf{RKOH$_{1,4}$}    & - & -         &     -   	& -         &  - 	 & -   & -  & - 			&  -  & - & 0     \\
	\midrule
	\textbf{ReCeNT$_2$}    &  0.08  & 0.3    & 0.1	 	&  0.16  	    	  &  0.02   &  0.4 & 0.01 & 0.16 & 0.13 & \textbf{1.0} & 1 \\

	\textbf{RIBL$_2$}    &  -            &   -		     &   0.0			&  0.68    &  -              &  -     & - & -   & 0.63 & 0.78 & 0   \\

	\textbf{HSAG$_2$}    &  -0.01          &   0.06	         &   0.1     	 	&  0.0          	  &  0.0       	  	  &  0.04      & 0.00 & 0.23  & 0.04 & 0.09 & 0  \\

	\textbf{WLST$_{2,5}$}    &     0.00    &   0.01      &   0.02	 	&  0.02        	  &     0.00 &  0.52  & \textbf{0.27} & 0.11   & -0.04 & 0.31 & 1  \\

	\textbf{WLST$_{2,10}$}    &      0.00      &  0.01       &   	 0.02	&  0.02        	  &     0.00  &  0.52  & 0.05 & 0.12  & -0.04 & 0.36 & 0   \\

	\textbf{V$_2$}    &    0.00     &   0.07     &     0.01 	 	&   0.00      	  &       0.00	  & 0.00 & 0.00 & 0.00  & 0.00 & 0.00 & 0  \\

	\textbf{VE$_2$}    & 0.00   &  0.00          &   0.01 	 	&  0.38   	  &   0.00  	  & \textbf{0.56}    & 0.00 & 0.00  & 0.00 & 0.53 & 1   \\

	\textbf{RKOH$_{2,2}$}    & -  & -         &  -      	& -         &  - 	 & -   & -  & - 			& -  & - & 0     \\

	\textbf{RKOH$_{2,4}$}    & - & -         &     -   	& -         &  - 	 & -   & -  & - 			&  -  & - & 0   \\
	\bottomrule
\end{tabular}
}

\end{table}



The results of the first experiment are summarised in Table~\ref{tab:ClustResults}.
The table contains ARI values obtained by the similarity measures for each data set and clustering algorithm used.
The last column of the table states the number of wins per approach.
The number of wins is calculated by simply counting the number of cases where the approach obtained the highest ARI value, a ''case'' being a combination of a data set and a clustering algorithm.
ReCeNT$_1$ wins 8 out of 10 times, and thus outperforms all other methods.
The best results are achieved in combination with spectral clustering, with exception being the TerroristAttack data set where WLST$_{1,*}$ and WLST$_{2,5}$ combined with hierarchical clustering achieved the highest ARI of 0.27, in contrast to 0.26 obtained by ReCeNT$_1$.
In all cases of the Mutagenesis and UWCSE data sets, ReCeNT$_1$ wins with a larger margin.
However, it is important to note that in the remaining cases, the closest competitor is not always the same.
In the case of IMDB data set in combination with spectral clustering, the closest competitor is VE$_1$ (together with RKOH$_{1,2}$), as well as in the case of WebKB in combination with spectral clustering.
In the cases of the TerroristAttack data set combined with the spectral clustering, the closest competitors are HSAG$_1$ and HSAG$_2$, while in the case with hierarchical clustering our approach is outperformed by WLST$_{1,*}$ and WLST$_{2,5}$.
These results show that the proposed similarity measure performs better over wide range of different tasks and biases, compared to the remaining approaches.
Moreover, when combined with the spectral clustering, ReCeNT$_1$ consistently performs well on all data sets, achieving the second-best result only on the TerroristAttack data set.



Each of the data sets exposes different bias, which influences the performance of the methods.
In order to successfully identify mutagenic compounds, one has to consider both attribute and link information, including the attributes of the neighbours.
Chemical compounds that have similar structure tend to have similar properties.
This data set is more suitable for \gls{ribl}, \gls{recent} and kernel approaches.
\gls{recent}$_1$ and \gls{ribl}$_1$ achieve the best results here\footnote{We were not able to make \gls{hs} work on this data set as it assumes edges between compound vertices which are non-existing in this data set}, while kernels approaches surprisingly do not perform better than the chance level.
The UW-CSE is a social-network-like data set where the task is to find two interacting  communities with different attribute-values - students and professors.
The distinction between two classes is made on a single attribute - professors have positions, while students do not, and the relation stating that professors advise students.
This task is suitable for \gls{hs} and \gls{hsag}.
However, both approaches are substantially outperformed by \gls{recent}$_1$ and \gls{cc}$_*$.
Similarly, the IMDB data set consists of a network of people and their roles in movies, which can be seen as a social network.
Here, directors can be differentiated from actors by a single edge type - actors work under directors which is explicitly encoded in the data set.
The type of interactions between entities matters the most, as it is not an attribute-rich data set, and is thus more suitable for methods that account for structural measures.
Accordingly, \gls{recent}, \gls{ribl}, \gls{wlst}$_{1,*}$ and VE  kernels achieve the best results.





The remaining data sets, WebKB and TerroristAttack, are entirely different in nature from the aforementioned ones.
These data set have a substantially larger number of attributes, but those are not sufficient to identify relevant clusters supported by labels, that is,  interactions contain important information.
Such bias is implicitly present in HS, and partially assumed by kernel approaches.
The results show that \gls{recent}$_1$ and \gls{wlst}$_{2,*}$ and VE$_2$ kernels achieve almost identical performance on the WebKB data set, while the remaining approaches are outperformed even by the baseline approach.
On the TerroristAttack data set, \gls{wlst}$_{1,*}$ kernel achieves the best performance, outperforming \gls{recent}$_1$ and HSAG$_1$.
Similarly to WebKB, other approaches are outperformed by the baseline approach.






The results summarised in Table \ref{tab:ClustResults} point to several conclusions.
Firstly, given that the proposed approach achieves the best results in 8 out of 10 test cases, the results suggest that it is indeed versatile enough to capture relevant information, regardless of whether  that comes from the attributes of vertices, their proximity, or connectedness  of vertices, even without parameter tuning.
Moreover, when combined with the spectral clustering, our approach consistently obtains good results on all data sets, while the competitor approaches achieve good results if the problem fits their bias.
Secondly, the results show that one has to consider not only the bias of the similarity measure, but the bias of the clustering algorithm as well, which is evident on most data sets where spectral clustering achieves substantially better performance than hierarchical clustering.
Finally, \gls{recent} and most of the approaches tend to be sensitive to the depth parameter, which is evident in the drastic difference in performance when different depths are used.
This suggests that increasing depth of a neighbourhood tree consequently introduces more noise.
Interestingly, while the results suggest that with \gls{recent} the depth of 1 performs the best, the performance of kernel methods tend to increase with the depth parameter.
These results justify the basic assumption of this approach that important information is contained in small local neighbourhoods.











\subsubsection{\textbf{(Q2) Relevance of components}}

\begin{table*}[t]
%\captionsetup{justification=centerlast}
\begin{center}
\footnotesize

\caption[Clustering performance of individual similarity components]{Performance of \gls{recent} with different parameter settings. The upper part of the table presents results with the neighbourhood trees with depth of 1, whereas the bottom part contains the results with depth set to 2. The parameters in italic indicate the best performance achieved.}

\begin{tabular}[t]{@{}lrrrrrrrrrr@{}}
	\toprule
	\textbf{Parameters} & \multicolumn{2}{r}{\textbf{Muta}} & \multicolumn{2}{r}{\textbf{UWCSE}} & \multicolumn{2}{r}{\textbf{WebKB}} & \multicolumn{2}{r}{\textbf{Terror}}  & \multicolumn{2}{r}{\textbf{IMDB}} \\
	\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
	                     &  Hier. & Spec    & Hier. & Spec & Hier.  & Spec  & Hier.  & Spec   & Hier.  & Spec   \\
	\midrule
	1,0,0,0,0   &  0.00	   &   0.00		 		& 0.25 & 0.2     		& 0.00  & 0.25       & 0.01 & 0.17 		& 0.05 & 0.05   \\

	0,1,0,0,0 &  	0.00  &  0.00     	 		& 0.52 & 0.12   		& 0.00  & 0.00     	  & 0.00 & -0.01		& 0.0 & 0.00  \\

	0,0,1,0,0  &  	0.00  &  0.00	      		& 0.05 & 0.1  	  		& 0.00 & 0.1         & 0.00 & 0.00  		& 0.14 & 0.13  \\

	0,0,0,1,0  & 0.30  &  0.30    				& 0.02 & -0.03 	   		& 0.00 & 0.2         & 0.00 & -0.01 		& 0.17 & 0.17  \\

	0,0,0,0,1  &    0.24  & 0.25				& 0.17 &  0.07     		& 0.00 & 0.02  		  & -0.01 & 0.00  		& 1.0 &   1.0   \\

	\textit{0.2,0.2,0.2,0.2,0.2} & 0.32 & 0.35 &0.96 &  0.86   		& 0.04 & 0.56 		  & 0.00 & 0.26 		& 0.62 & 1.0  \\
	\midrule
	1,0,0,0,0  &  	0.00   & 0.00	 			& 0.00 & 0.2   			& 0.00 & 0.27 		  & 0.00 & 0.17  		& 0.05 & -0.05   \\

	0,1,0,0,0 &  	0.00  & 0.00	 			& 0.03 & 0.16 			& 0.00	& 0.00 		  & 0.00 & -0.01 		& 0.0 & 0.00  \\

	0,0,1,0,0  &  	0.00 &  0.00  				& 0.00 & 0.08  	  		& 0.00 & 0.01      	  & 0.01 & 0.00 		& 0.15 & 0.13  \\

	0,0,0,1,0  &  	0.29 &  0.29   				& 0.01 & -0.03			& 0.02  & 0.2 	  	  & -0.01 & -0.01		& 0.00 & 0.00  \\

	0,0,0,0,1  &   0.00 & 0.27     			    & 0.03 	& -0.04			& 0.00 & 0.02  		  & 0.00 &  0.00		& 1.0 &  1.0    \\

	\textit{0.2,0.2,0.2,0.2,0.2} & 0.08 & 0.3  & 0.1	&  0.07  	    &  0.02&  0.4 		  & 0.01 & 0.16 		& 0.13 & 1.0 \\
	\bottomrule

\end{tabular}
\label{tab:RResultsComponenets}

\end{center}
\end{table*}

In the second experiment, we evaluate how relevant each of the five components in Equation~\ref{eq:Sim} is.
Table~\ref{tab:RResultsComponenets} summarises the results.
There are only two cases (Mutagenesis and IMDB) where using a single component (if it is the right one!) suffices to get results comparable to using all components (Table~\ref{tab:RResultsComponenets}).
This confirms that clustering relational data is difficult not only because one needs to choose the right source of similarity, but also because the similarity of relational objects may come from multiple sources, and one has to take all these into account in order to discover interesting clusters.




These results may explain why \gls{recent} almost consistently outperforms all other methods in the first experiment.
First, \gls{recent} considers different sources of relational similarity; and second, it ensures that each source has a comparable impact (by normalising the impact of each source and giving each an equal weight in the linear combination).
This guarantees that if a component contains useful information, it is taken into account.
If a component has no useful information, it adds some noise to the similarity measure, but the clustering process seems quite resilient to this.
If {\em most} of the components are irrelevant, the noise can dominate the pattern.
This is likely what happens in experiment 1 when depth 2 neighbourhood trees are used: too much irrelevant information is introduced at level two, dominating the signal at level one.
%Not having these two properties also explains why other approaches perform well only on the tasks that suit their biases.

%These results suggest that the main benefit of ReCeNT comes from (1) consideration of several sources of relational similarity, and (2) decoupling those individual sources in to a linear combination.
%By doing this, ReCeNT is able to extract useful information even if it is present only in a subset of the components.
%A possible explanation might be the following one: if an individual component does not contain any pattern or important information, it behaves as addition of random noise to the remaining components (assuming they contain important information).
%Hence, it follows the assumption underlying every machine learning algorithm: the data is composed of a pattern and random noise.
%Unfortunately, this also means that if most of the components are irrelevant, the noise can dominate the pattern.
%The latter case is evident in the results of the experiment 1 when the depth of 2 was used for the neighbourhood trees, which are substantially worse than the results with the depth of 1.
%Not having these two properties also explains why other approaches perform well only on the tasks that suit their biases.









\subsubsection{\textbf{(Q3) Learning weights in an unsupervised manner}}

The first experiment shows that \gls{recent} outperforms the competitor methods even without parameters being tuned.
The second experiment shows that one typically has to consider multiple interpretations of similarity in order to obtain a useful clustering.
A natural question to ask is whether the  parameters could be learned from data in an unsupervised way.
The possibility of tuning offers an additional flexibility to the user.
If the knowledge about the right bias is available in advance, one can specify it through adjusting the parameters of the similarity measure, potentially achieving even better results than those presented in Table \ref{tab:ClustResults}.
However, tuning the weights in an automated and systematic way is a difficult task as there is no clear objective function to optimise in a purely unsupervised settings.
Many clustering evaluation criteria, such as ARI, require a reference clustering which is not available during clustering itself.  Other clustering quality measures do not require a reference clustering, but each of those has its own bias \cite{VanCraenendonck15}.


\begin{table}
    \centering
    \caption[Affinity aggregation results]{\textbf{Affinity aggregation results.} The subscript indicates the depth of the neighbourhood tree.}
    \begin{tabular}{@{}cccccc@{}}
        \toprule
        \textbf{Approach} & \textbf{IMDB} & \textbf{UWCSE} & \textbf{Mutagenesis} & \textbf{WebKB} & \textbf{Terror} \\
        \midrule
        ReCeNT$_{1}$        & 1.0           & 0.98           & 0.35          & 0.56           & 0.26             \\

        AASC$_{1}$        & 0.78          & 0.65           & 0.35          & 0.57           & 0.28              \\
        \midrule
        ReCeNT$_{2}$        & 1.0           & 0.07           & 0.3           & 0.4            & 0.16             \\

        AASC$_{2}$        & 0.67          & 0.23           & 0.3           & 0.4            & 0.23              \\
        \bottomrule

    \end{tabular}

    \label{tab:withWeights}
\end{table}


An approach that might help in this direction is the \textit{Affinity Aggregation for Spectral Clustering} (AASC) \cite{HuangCC12}.
This work extends spectral clustering to a multiple affinity case.
The authors start from the position that similarity of objects often can be measured in multiple ways, and it is often difficult  to know in advance how different similarities should be combined in order to achieve the best results.
Thus, the authors introduce an approach that learns the weights that would, when clustered into the desired number of clusters, yield the highest intra-cluster similarity.
That is achieved by iteratively optimising: (1) the cluster assignment given the fixed weights, and (2) weights given a fixed cluster assignment.
Thus, by treating each component in Equation \ref{eq:Sim} as a separate affinity matrix, this approach tries to learn their optimal combination.





We have tried AASC in \gls{recent}, and the results  are summarised in Table~ \ref{tab:withWeights}.
These results lead to several conclusions.
Firstly, in most cases AASC yields no substantial benefit or even hurts performance.
This confirms that learning the appropriate bias (and the corresponding parameters) in an entirely unsupervised way is a difficult problem.
The main exceptions are found for depth 2: here, a substantial improvement is found for UWCSE and TerroristAttack.
This seems to indicate that the bad performance on depth 2 is indeed due to an overload of irrelevant information, and that AASC is able to weed out some of that.
Still, the obtained results for depth 2 are not comparable to the ones obtained for depth 1.
We conclude that tuning the weights in an unsupervised manner will require more sophisticated methods than the current state of the art.









\subsubsection{\textbf{(Q4) Performance in a supervised setting}}


The previous experiments point out that the proposed dissimilarity measure performs well compared to the existing approaches, but finding the appropriate weights is difficult.
Though our focus is on clustering tasks, we can use our dissimilarity measure for classification tasks as well.
The availability of labels offers a clear objective to optimise when learning the weights, and thus allows us to evaluate the appropriateness of \gls{recent} for classification.

We have set up an experiment where we use a $k$ nearest neighbours (kNN) classifier with each of the (dis)similarity measures.
It consists of a 10-fold cross-validation, where within
each training fold, an internal 10-fold cross-validation is used to tune the parameters of the similarity measure, and kNN with the tuned similarity measure is next used to classify the examples in the corresponding test fold.



\begin{table}
%\captionsetup{justification=centerlast}
	\begin{center}
		\small
		\caption[Performance of the kNN classifier with different (dis)similarity measures.]{Performance of the kNN classifier with different (dis)similarity measures and weight learning. The performance is expressed in terms of accuracy over the 10-fold cross validation. }
			\label{tab:SupervisedRes}
		\begin{tabular}[htb]{@{}cccccc@{}}
		\toprule
		\textbf{Approach} & \textbf{IMDB}   & \textbf{UWCSE}& \textbf{Mutagenesis}  & \textbf{WebKB}    &  \textbf{Terrorists} \\
		\midrule
		\gls{hs} 		 		  &	88.08	        &	76.66       &  0.00                 &		12.78       &   	27.51		\\

		\gls{cc}		    	  &	88.08	        &\textbf{99.85} &  60.08                &		61.07       &   	38.28	\\

		\gls{hsag}     		  &	88.08	        &	95.88       &  77.40	            &		12.82       &   	75.62		\\

		\gls{recent}     	  &	\textbf{100}	&\textbf{100}	&  \textbf{85.54 }      &	\textbf{100}    &   	\textbf{85.60}	\\

		\gls{ribl}	    	  &	\textbf{100}	&	77.22       &  76.37                &		84.11       &   	-		\\

		\gls{wlst}      	      &	93.60	        &	44.94       &  76.37	            &		47.35       &   	45.56		\\

		VE			      &	\textbf{100}	&	98.26       &  70.60	            &		49.33       &   	30.00		\\

	    V 			      &	93.80	        &	43.61       &  70.42                &		47.35       &   	44.39		\\

		\gls{rkoh}	          &	95.07	        &	67.26       &  60.78                &       -	        &       - 		\\
		\bottomrule
		\end{tabular}

	\end{center}

\end{table}



The results of this experiment are summarised in Table \ref{tab:SupervisedRes}.
ReCeNT achieves the best performance on all data sets.
On the IMDB data set, \gls{recent} achieves perfect performance, as do RIBL and VE.  On UWCSE, \gls{recent} is 100\% accurate; its closest competitor, \gls{cc}, achieves 99.85\%.
From the classification viewpoint, these two data sets are easy: the classes are differentiable by one particular attribute or relation.
%Thus, high performance of most (dis)similarity measure is not surprising, especially in the case of IMDB.
On Mutagenesis and Terrorists, the difference is more pronounced: \gls{recent} achieves around 85\% accuracy, with its closest competitor (\gls{hsag}) achieving 76\% or 77\%.
On WebKB, finally, \gls{recent} and \gls{ribl} substantially outperform all the other approaches, with \gls{recent} achieving 100\% and \gls{ribl} 84.11\%.

The remarkable performance of \gls{recent} on WebKB is explained by inspecting the tuned weights.  These reveal that \gls{recent}'s ability to jointly consider vertex identity, edge type distribution, and vertex attributes (in this case, words on webpages) are the reason why it performs so well.  None of the other approaches take all three components into account, which is why they achieve substantially worse results.

These results clearly show that accounting for several views of similarity is beneficial for relational learning.
Moreover, the availability of labelled information is clearly helpful and \gls{recent} is capable of successfully adapting its bias towards the needs of the data set.





\subsubsection{\textbf{(Q5) Runtime comparison}}

\begin{table}
%\captionsetup{justification=centerlast}
	\begin{center}
		\small
		\caption[Runtime comparison of various relational clustering approaches]{Runtime comparison in minutes (rounded up to the closest integer). The runtimes include the construction of supporting structures and time needed to calculate a similarity between each pair of vertices in a given hyper-graph. Note that graph kernel measures (in italic) are obtained using the external software provided with \cite{NIPS2015_5688}. - indicates that the calculation took more than 24 hours. }
			\label{tab:Runtimes}
		\begin{tabular}[htb]{@{}cccccc@{}}
		\toprule
		\textbf{Approach } & \textbf{IMDB}     & \textbf{UWCSE} &   \textbf{Mutagenesis}   &   \textbf{WebKB}   &  \textbf{Terror} \\
		\midrule
		\gls{hs} 		 		&	1		&	1	   &  	-	  &		1	   &   	1		\\

		\gls{cc}$_2$ 			&	1		&	1      &  	1	  &		5	   &   	1		\\

		\gls{cc}$_4$ 			&	1		&	1	   &  	1	  &		8	   &   	8		\\

		\gls{hsag}$_1$ 		&	1		&	1	   &  1		  &		2	   &   	2		\\

		\gls{hsag}$_2$ 		&	1		&	1	   &  1		  &		5	   &   	2		\\

		\gls{recent}$_1$		&	1		&	1	   &  	1	  &		2	   &   	2		\\

		\gls{recent}$_2$		&	1		&	1	   &  	3	  &		10	   &   	5		\\

		\gls{ribl}$_1$		&	1		&	2	   &  	540	  &		1320   &   	-		\\

		\gls{ribl}$_2$		&	2		&	5	   &  -	  &		-	   &   	-		\\

		$\gls{wlst}_{1,5}$	&	1		&	1	   &  	1	  &		1	   &   	1		\\

		$\gls{wlst}_{1,10}$	&	1		&	1	   &  	1	  &		1	   &   	1		\\

		$\gls{wlst}_{2,5}$	&	1		&	1	   &  	1	  &		4	   &   	5		\\

		$\gls{wlst}_{2,10}$	&	1		&	1	   &  	1	  &		4	   &   	5		\\

		$VE_1$			&	1		&	1	   &  	1	  &		1	   &   	2		\\

		\gls{rkoh}$_{1,2}$	&	1		&	2	   &  	10	  &	-		   &   -			\\

		\gls{rkoh}$_{1,4}$	&	-		&	-	   &  - 	  &  -	   &  - 		\\

		\gls{rkoh}$_{2,2}$	&	-		&	-	   &  - 	  &  -	   &  - 		\\

		\gls{rkoh}$_{2,4}$	&	-		&	-	   &  - 	  &  -	   &  - 		\\
		\bottomrule
		\end{tabular}

	\end{center}

\end{table}

Table \ref{tab:Runtimes} presents a comparison of runtimes for each approach.
All the experiments were run on a computer with 3.20 GHz of CPU power and 32 GB RAM.
The runtimes include the construction of supporting structures (neighbourhood trees and context descriptors), calculation of similarity between all pairs of vertices, and clustering.
The measured runtimes are consistent with the previously discussed complexities of the approaches.
\gls{hs}, \gls{hsag}, \gls{cc}, \gls{recent} and kernel approaches (excluding \gls{rkoh}) are substantially more efficient than the remaining approaches.
This is not surprising, as \gls{hs}, \gls{hsag} and \gls{cc}  use very limited information.
It is, however, interesting to see that ReCeNT and WLST,  which use substantially more information, take only slightly more time to compute, while achieving substantially better performance on most data sets.
These approaches are also orders of magnitude more efficient than \gls{ribl} and \gls{rkoh}, which did not complete on most data sets with depth set to 2.
That is particularly the case for RKOH which did not complete in 24 hours even with the depth of 1, when the walk length was set to 4.


\begin{tcolorbox}[enhanced,attach boxed title to top center={yshift=-3mm,yshifttext=-1mm}, colback=blue!5!white, colframe=blue!75!black, colbacktitle=white, coltitle=red!25!black,
title=Intermezzo: Semi-supervised clustering with COBRAS,fonttitle=\bfseries, boxed title style={size=small,colframe=blue!75!black} ]

	Clustering is a difficult task due to the lack of a clear criterion for evaluating clusterings.
	\textit{Semi-supervised clustering} starts from a position that, even though a fully labelled dataset is not available, the user is able to provide a limited amount of feedback that can be further used to guide the clustering procedure.
	The feedback usually takes the form of the following question: \textit{do instances i and j have the same cluster label in the target clustering?} Answering the question with \texttt{yes} yields a \textit{must-link} constraint and a \textit{cannot-link} constraint otherwise. \\


	In a series of works on \texttt{COBRAS}, Van Craenendonck et al.~\cite{DBLP:conf/ijcai/CraenendonckDB17,DBLP:journals/corr/abs-1803-11060,DBLP:journals/corr/abs-1805-00779} introduce a fast, iterative and active clustering method with pairwise constraints.
	\texttt{COBRAS} is based on the concept of \textit{super-instances}: a local set of instances that are assumed to belong to the same clustering in the final target clustering.
  The core idea of \texttt{COBRAS} is to ask the pairwise queries among the representatives of super-instances instead of the individual data points.
	Thus, it is related to the idea of \textit{lifted inference}~\cite{Poole:2003:FPI:1630659.1630801}. \\


	\texttt{COBRAS} introduces an incremental procedure that, starting with all data points in a single super-instance, repeatedly intertwines the following steps:
	\begin{itemize}
		\item select a super-instance
		\item repeatedly split the selected super-instance (and its descendants) in two, asking a \textit{pairwise query} between the descendants. Stop when the first \textit{must-link} constraint is obtained
		\item Ask the \textit{pairwise query} among all available super-instances, starting with the closest ones.
	\end{itemize}
	This has proven to be a fast, versatile and well-performing semi-supervised clustering procedure that outperforms many of the existing semi-supervised clustering approaches on a wide range of benchmark tasks, and \textit{merge} super-instances connected by the \textit{must-link} constraint.


\end{tcolorbox}






\section{Conclusion}

We have introduced a novel dissimilarity measure for clustering relational objects, based on a hyper-graph interpretation of a relational data set.
In contrast with the previous approaches, our approach takes multiple aspects of relational similarity into account, and  allows one to focus on a specific vertex type of interest, while at the same time leveraging the information contained in other vertices.
We develop the dissimilarity measure to be versatile enough to capture relevant information, regardless whether it comes from attributes, proximity or connectedness in a hyper-graph.
To make our approach efficient, we introduce neighbourhood trees, a structure to compactly represent the distribution of attributes and hyper-edges in the neighbourhood of a vertex.


We experimentally evaluate our approach on several data sets on both clustering and classification tasks.
The experiments show that the proposed method often achieves better results than the competitor methods with regards to the quality of clustering and classification, showing that it indeed is versatile enough to adapt to each data set individually.
Moreover, the proposed approach, though more expressive, is as efficient as the state-of-the-art approaches.
One open challenge is to which extent the parameters of the proposed similarity measure can be learnt from data in an unsupervised (or a semi-supervised) way.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Keep the following \cleardoublepage at the end of this file,
% otherwise \includeonly includes empty pages.
\cleardoublepage

% vim: tw=70 nocindent expandtab foldmethod=marker foldmarker={{{}{,}{}}}
