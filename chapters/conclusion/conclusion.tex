\chapter{Conclusion}\label{ch:conclusion}


We conclude by summarising the presented work, restating its main contributions,and providing an outlook on future research.



\section{Thesis summary and contributions}


The success of any machine learning application depends on the quality of the provided data.
It reflects the quality of the data collection process and data representation.
The quality of data representation is reflected in the quality of features that are used to describe the desired objects, which should not only the informative and relevant but should also allow capturing interesting patterns easily.
This usually requires a domain expert knowing which features might be relevant for the task.
What used to be a time-consuming and labour-intensive task is now transformed into an automatic procedure by \textit{representation} or \textit{deep learning}.
The field of representation learning concerns the development of techniques that automatically extract an effective representation of the provided data and has revolutionised the field of machine learning over the last decade.


Unfortunately,  the majority of the existing representation learning methods focuses on \textit{flat data}, which represents data with vectors containing features values.
This is in contrast with the real world which contains objects and their mutual relationships, i.e., the real world is \textit{inherently relational}.
The existing representation learning approaches addressing the relational data typically resort to \textit{flattening} the relational data into vectors.
This mapping can only \textit{approximate} the relational data, and we currently lack a quantitative measure to assess the quality of the approximation.
Moreover, these approaches do not leverage the tools developed within \textit{statistical relational learning} which concerns learning from relational data.
Statistical relational learning leverages the expressive power of first-order logic to represent such complex data which makes it amongst the most powerful machine learning frameworks.



This thesis serves as an exploration of ideas that combine representation learning with statistical relational learning that, instead of resorting to data flattening, rely on first-order logic as a representation language for both data representation and latent feature representation.
This thesis contributes several algorithms, techniques and analyses towards achieving this goal.
We now summarise the main contributions of the thesis.


\textbf{Expressive relational clustering framework}
The first contribution is a novel relational clustering framework.
The main novelty of the proposed framework is a versatile dissimilarity measure that does not impose a fixed view on what makes relational objects similar, but is a \textit{composition} of several \textit{primitive} similarities.
These primitive similarities include attribute-based similarity, proximity and various similarities of neighbourhoods.
The experiments show that accounting for a diverse set of primitive similarities is beneficial for both relational clustering and classification, outperforming the  approaches that  consider only one (or a few) of the primitive similarities.


\textbf{Exploiting symmetries to define relational latent features}
The second contribution is the technique for inventing relational latent representations by capturing approximate symmetries in data.
The proposed technique relies on the previously introduced relational clustering framework to identify such symmetries by means of clustering.
The main novelty this technique introduces is that it clusters objects and relationships in a data using different \textit{similarity interpretations}, instead of composing several of them into one joint complex similarity measure.
Compared to the existing work that uses clustering to enhance relational representations, the proposed technique clusters the instances of relationships, not just their types.
It can thus discover various \textit{sub-types} of relationships that might carry useful information.
We show that learning from the latent representations created by clustering is beneficial for relational learning as it often improves the performance of the relational classifier while at the same time reducing the complexity of the induced model.
Moreover, we introduce a relatively simple method for explaining the invented latent features which allow us to overcome the typical issue of a black-box feature invention machines.



These two contributions motive us to pose the following claim:

\begin{quote}
	\textbf{Claim.} To successfully address relational learning tasks, one has to account for several sources of relevant information.

\end{quote}


The results related to the first two contributions show that the reason why relational learning is difficult is that relevant information comes from different sources, being it the attributes or surrounding structure and that separating those sources helps to improve the performance.
This is related to the concept of \textit{disentangling the factors of variation} many deep learning approaches aspire to achieve.




\textbf{Auto-encoding logic programs}
The third contribution of this thesis are auto-encoding logic programs.
They stand for a generalisation of the flat auto-encoder towards logic programs as a representation language.
That includes both data and the computational framework for defining relational features.
In contrast to \gls{curled} which introduces relational concepts into relational representation learning but still relies on statistical summaries for inventing the latent features, Auto-encoding logic program rely on clausal logic for every step of the pipeline.
Therefore, they fully leverage the expressiveness of logic.
We introduce a constraint optimisation framework for learning auto-encoding logic programs that proved to be reasonably efficient.
The experiments show that latent representations created by \alp{} aid learning generative \gls{srl} models, resulting in a better performance of the \gls{srl} models learnt in the latent space.







\textbf{Analysis of relational representation learning paradigms}
The fourth contribution of this thesis is an analysis of relational representation learning techniques.
We start by analysing the relational latent representations created by \gls{curled} and show that such representations are effective because the identify groupings in data that correspond well with the available labels.
We then focus on comparing the symbolic and embedding \gls{srl} approaches on a set of standard benchmarks for relational classification and knowledge base completion.
The results of the comparison show that embedding approaches are suitable for scenarios when the data is \textit{curated} and most of the available information is found predictive for the task at hand.
Symbolic approach like \gls{srl} are more suitable for the case when data cannot be curated and reduced to only the relevant information.
Finally, embeddings \gls{srl} approaches seem to be a good alternative for assessing the similarity in relational data.



In summary, we contribute novel methods for representation learning with relational data.
The methods introduced in this thesis are \textit{symbolic} in their nature, which is in stark contrast with the existing methods which rely on gradient-based optimisation.
This allows them to leverage a powerful representation language of clausal logic to learn more expressive latent features.
Our second main contribution is a better understanding of the strengths and weaknesses of symbolic and embeddings representation learning approaches.








\section{Future work}

We believe the methods introduced in this thesis clearly demonstrate the benefit of retaining logic as a representation language for relational representation learning.
The most important benefit is the expressiveness of predicate logic that can represent very complicated interactions in data, various forms of reasoning while at the same time being interpretable.
We hope these results will inspire others to pursue this direction of research.
Here we discuss directions for future research, grouping them in the theoretical and practical aspects.


\subsection{Theoretical aspects}


\paragraph{Properties of good relational data representations}
\textit{What constitutes a good data representation} is one of the pressing open  questions in this line of research.
Answering this question is the critical step that will shape the development of novel relational representation techniques.
As we have outlined in Chapter \ref{ch:learningrepresentations}, we do have a good intuition what we consider to be an effective \textit{propositional} representation (Section \ref{ch3:sec:properties} in Chapter \ref{ch:learningrepresentations}): hierarchically organised, smooth, sparse and so on.
Most of these properties are intuitions that come from empirical observation, not strong theoretical results.
Moreover, this thesis has proven that mapping those concepts to relational data representations is not a straightforward task.
Part of the reason is the semantic difference between Euclidean spaces and first-order logic.
Understanding which data properties help relational learning will help us tune the latent representation created by \gls{curled} and \alp{s}.
This is especially important for \alp{s} as it can inform us how to impose the language bias in order to discover the latent features.





\paragraph{Different representation learning techniques}
This thesis has explored two representation learning approaches -- \textit{learning by clustering} and the \textit{auto-encoding principle}.
The literature on representation learning consists of many different, and effective, ideas.
It is well worth exploring various ideas, and one particularly interesting idea are \textit{Generative adversarial networks}~\cite{Goodfellow:2014:GAN:2969033.2969125}, which learns a generative latent model through a competition with a \textit{discriminator} network trying to distinguish between the real data and the samples of the generative model.
One of the main challenges here is how to propagate the feedback from the discriminator to the generator network, and the bi-directionality of Prolog predicates might be an elegant way to tackle this.


\paragraph{Do we need probabilistic latent representations?}
In this thesis, we have treated relational representation learning as a deterministic mapping from the original data space to the latent one.
Thus,  we have ignored the probabilistic aspect of \gls{srl} methods.
An interesting question for the future research is whether it is necessary to include the probabilistic reasoning in the mapping from the original to the latent representation.
It is certain that including the probabilistic component would impact the efficiency of learning, but we cannot state for sure that it would allow us to construct substantially better representations because the relational data is still deterministic.
However, such probabilistic relational representation might be beneficial, and necessary, for the case of probabilistic data, e.g., often used in Problog~\cite{DeRaedt:2007:PPP:1625275.1625673}.




\paragraph{Integrating symbolic and sub-symbolic representation learning methods}
This thesis considers learning relational latent representations from the perspective of logical approaches to \gls{srl}, and is focused on learning latent representation of symbolic data.
Many existing approaches for representation learning considers numerical and signal-like data and effectively treat the symbolic data in the same way by re-representing it in the Euclidean space.
This sacrifices many aspects and expressivity of logic.
We believe the middle ground, which exploits the best parts of both logic and neural networks, is the direction that could show the most advancement.
A good step in that direction is \textit{DeepProbLog}~\cite{DBLP:journals/corr/abs-1805-10872}, which elegantly integrates high level reasoning capabilities of logic with the perception achieved with \gls{ann}s.
We believe another avenue worth exploring is a development of \textit{hybrid} learning that would exploit strengths of both symbolic and distributional approaches for relational learning, similar in direction to differentiable theorem proving~\cite{DTP2017}, but without sacrificing the expressivity of individual components.






\subsection{Practical aspects}


\paragraph{Scalable learning of relational representations}
This thesis considers laying down the foundations for a logic-based relational representation learning, and a lot of work still has to be done in that direction.
But making these methods applicable to real-world scenarios, the scalability is an important issue to address.
We do not pay special attention to this issue during the thesis.
Improving the scalability of \gls{curled} involves investigating scalable clustering methods, as the calculation of the similarity measure is not the most computationally expensive step in the pipeline.
\alp{s}, on the other hand, can benefit from improving the constraint optimisation procedure by exploring novel problem encodings, decomposition techniques~\cite{decompositions,Friesen:2015:RDN:2832249.2832284}  and new semantic constraints.



\paragraph{Task-specific representations}
The goal of learning latent representations of data is currently focused on improving the performance of a predictive model, either through improving the predictive accuracy of the model or making the learning easier.
This is the setup we have focused in this thesis.
However, there is no reason to stop there: having a model of a domain, i.e., facts and clauses that are known to be true, \gls{srl} systems can reason about any aspect of the domain.
We believe this open many doors for relational representation learning, where one interesting avenue might be to learn representations for faster inference and tackling the challenge of \textit{lifted inference}~\cite{VdBThesis13,Poole:2003:FPI:1630659.1630801}.


\paragraph{Learning how to \textit{see} relational data}
Perhaps the most important idea from Chapters \ref{ch:clustering} and \ref{ch:symmetries} is that explicitly accounting for multiple sources of information within relational data can substantially improve the performance.
However, in both chapters we have relied on manually specifying these source; for clustering in Chapter \ref{ch:clustering} we have used all sources in a linear combination, while in Chapter \ref{ch:symmetries} we have decided to use the attribute information, neighbourhood structures and the combination of both.
An interesting issue to address is whether we could develop methods to identify these sources automatically.
Our relational representation learning methods \gls{curled} would benefit substantially from this development as it could potentially discover more complex latent features without an intervention of the user.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Keep the following \cleardoublepage at the end of this file,
% otherwise \includeonly includes empty pages.
\cleardoublepage

% vim: tw=70 nocindent expandtab foldmethod=marker foldmarker={{{}{,}{}}}
