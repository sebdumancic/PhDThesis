\chapter{Conclusion}\label{ch:conclusion}


We conclude by summarising the presented work, restating its main contributions,and providing an outlook on future research.



\section{Thesis summary and contributions}


The success of any machine learning application depends on the quality of provided data.
That does not only  reflect the quality of the data collection process, but the quality of data representation as well.
The quality of data representation is reflected in the quality of features that are used to describe the desired objects, which should not only the informative and relevant, but should also allow to capture interesting patterns easily.
This usually requires a domain expert knowing which features might be relevant for the task.
What used to be a time-consuming and labour-intensive task is now transformed into an automatic procedure by \textit{representation} or \textit{deep learning}.
The field of representation learning concerns the development of techniques that automatically extract effective representation of the provided data and has revolutionised the field of machine learning over the last decade.


Unfortunately,  the majority of the existing representation learning methods focuses on \textit{flat data}, which represents data with vectors containing features values.
This is in contrast with the real world which contains objects and their mutual relationships, i. e., the real world is \textit{inherently relational}.
The existing representation learning approaches addressing the relational data typically resort to \textit{flattening} the relational data into vectors.
This mapping can only \textit{approximate} the relational data, and we currently lack a quantitative measure to assess the quality of the approximation.
Moreover, these approaches do not leverage the tools developed within \textit{statistical relational learning} which concerns learning from relational data.
Statistical relational learning leverages the expressive power of first-order logic to represent such complex data which makes it amongst the most powerful machine learning frameworks.



This thesis serves as an exploration of ideas that combine representation learning with statistical relational learning that, instead of resorting to data flattening, rely on first-order logic as a representation language for both data representation and latent feature representation.
This thesis contributes several algorithms, techniques and analyses towards achieving this goal.
We now summarise the main contributions of the thesis.


\textbf{Expressive relational clustering framework}
The first contribution is a novel relational clustering framework.
The main novelty of the proposed framework is a versatile dissimilarity measure that does not impose a fixed view on what makes relational objects similar, but is a \textit{composition} of several \textit{primitive} similarities.
These primitive similarities include attribute-based similarity, proximity and various similarities of neighbourhoods.
The experiments show that accounting for a diverse set of primitive similarities is beneficial for both relational clustering and classification, outperforming the  approaches that  consider only one (or a few) of the primitive similarities.


\textbf{Exploiting symmetries to define relational latent features}
The second contribution is the technique for inventing relational latent representations by capturing approximate symmetries in data.
The proposed technique relies on the previously introduced relational clustering framework to identify such symmetries by means of clustering.
The main novelty this technique introduces is that it clusters objects and relationships in a data using different \textit{similarity interpretations}, instead of composing several of them into one joint complex similarity measure.
Compared to the existing work that uses clustering to enhance relational representations, the proposed technique clustering the instances of relationships, not just their types.
It can thus discover various \textit{sub-types} of relationships that might carry useful information.
We show that learning from the latent representations created by clustering is beneficial for relational learning as it often improves the performance of the relational classifier while at the same time reduces the complexity of the induced model.
Moreover, we introduce a relatively simple methods for explaining the invented latent features which allow us to overcome the typical issue of a black-box feature invention machines.



These two contributions motive us to pose the following claim:

\begin{quote}
	\textbf{Claim.} To successfully address relational learning tasks, one has to account for several sources of relevant information.

\end{quote}


The results related to the first two contributions show that the reason why relational learning is difficult is that relevant information comes from different sources, being it the attributes or surrounding structure, and that separating those sources helps to improve the performance.




\textbf{Auto-encoding logic programs}
The third contribution of this thesis are auto-encoding logic programs.
They stand for a generalisation of the flat auto-encoder towards logic programs as a representation language.
that includes both data and the computational framework for defining relational features.
In contrast to \gls{curled} which introduces relational concepts into relational representation learning but still relies on statistical summaries for inventing the latent features, Auto-encoding logic program rely on clausal logic for every step of the pipeline.
Therefore, they fully leverage the representation power of logic.
We introduce a constraint optimisation framework for learning auto-encoding logic programs that proved to be reasonable efficient.
The experiments show that latent representations created by \alp{} aid learning generative \gls{srl} models, resulting in a better performance of the \gls{srl} models learnt in the latent space.







\textbf{Analysis of relational representation learning paradigms}
The fourth contribution of this thesis is an analysis of relational representation learning techniques.
We start by analysing the relational latent representations created by \gls{curled} and show that such representations are effective because the identify groupings in data that correspond well with the available labels.
We then focus on comparing the symbolic and embedding \gls{srl} approaches on a set of standard benchmarks for relational classification and knowledge base completion.
The results of the comparison show that embedding approach are suitable for scenarios when the data is \textit{curated} and most of the available information is found relevant for the task at hand, while symbolic \gls{srl} approaches are more suitable for the case when data cannot be curated and reduced to only the relevant information.
When compared on the standard knowledge base completion tasks, symbolic \gls{srl} methods outperform embeddings approaches by a large margin.
Finally, embeddings \gls{srl} approaches seem to be a good alternative for assessing the similarity in relational data.



In summary, we contribute novel methods for representation learning with relational data.
The methods introduced in this thesis are \textit{symbolic} in their nature, which is in stark contrast with the existing methods which rely on gradient-based optimisation.
This allows them to leverage a powerful representation language of clausal logic to learn more expressive latent features.
Our second main contribution is a better understanding of the strengths and weaknesses of symbolic and embeddings representation learning approaches.








\section{Future work}

We believe the methods introduced in this thesis clearly demonstrate the benefit of retaining logic as a representation language for relational representation learning.
The most important benefit is the expressiveness of predicate logic that can represent very complicated interactions in data, various forms of reasoning while at the same time being interpretable.
We hope these results will inspire others to pursue this direction of research.
Here we discuss directions for future research, grouping them in the theoretical and practical aspects.


\subsection{Theoretical aspects}


\textbf{Properties of good relational data representations}
\textit{What constitutes a good data representation} is one of the pressing open  questions in this line of research.
Answering this question is the critical step that will shape the development of novel relational representation techniques.
As we have outlined in Chapter \ref{ch:learningrepresentations}, we do have a good intuition what we consider to be an effective \textit{propositional} representation (Section \ref{ch3:sec:properties} in Chapter \ref{ch:learningrepresentations}): hierarchically organised, smooth, sparse and so on.
Most of these properties are intuitions that come from empirical observation, not strong theoretical results.
Moreover, this thesis has proven that mapping those concepts to relational data representations is not a straightforward task.
Part of the reason is the semantic difference between Euclidean spaces and first-order logic.
Understanding which data properties help relational learning will help use tune the latent representation created by \gls{curled} and \alp{s}.
This is especially important for \alp{s} as it can inform us how to impose the language bias in order to discover the latent features.





\textbf{Different representation learning techniques}
This thesis as explored two representation learning approaches -- \textit{learning by clustering} and the \textit{auto-encoding principle}.
The literature on representation learning consists of many different, and effective, ideas.
It is well worth exploring various ideas, and one particularly interesting are \textit{Generative adversarial networks}~\cite{Goodfellow:2014:GAN:2969033.2969125}, which learns a generative latent model through a competition with a \textit{discriminator} network trying to distinguish between the real data and the samples of the generative model.
One of the main challenges here is how to propagate the feedback from the discriminator to the generator network, and the bi-directionality of Prolog predicates might be an elegant way to tackle this.



\textbf{Integrating symbolic and sub-symbolic representation learning methods}
This thesis considers learning relational latent representations from the perspective of logical approaches to \gls{srl}, and is focus on learning latent representation of symbolic data.
Many existing approaches for representation learning considers numerical and signal-like data and effectively treat the symbolic data in the same way by re-representing it in the Euclidean space.
This sacrifices many aspects and expressivity of logic.
We believe the middle ground, which exploits the best parts of both logic and neural networks, is the direction that could show the most advancement.
A good step in that direction is \textit{DeepProbLog}~\cite{DBLP:journals/corr/abs-1805-10872}.



\subsection{Practical aspects}


\textbf{Scalable learning of relational representations}
This thesis considers laying down the foundations for a logic-based relational representation learning, and a lot of work still has to be done in that direction.
But making these methods applicable to real-world scenarios, the scalability is an important issue to address.
We do not pay special attention to this issue during the thesis.
Improving the scalability of \gls{curled} involves investigating scalable clustering methods, as calculation of the similarity measure is not the most computationally expensive step in the pipeline.
\alp{s}, on the other hand, can benefit from improving the constraint optimisation procedure by exploring novel problem encodings, decomposition techniques~\cite{decompositions,Friesen:2015:RDN:2832249.2832284}  and new semantic constraints.



\textbf{Task-specific representations}
The goal of learning latent representations of data is currently focused on improving the performance of of a predictive model, either through improving the predictive accuracy of the model or making the learning easier.
This is the setup we have focused in this thesis.
However, there is no reason to stop there: having a model of a domain, i.e., facts and clauses that are known to be true, \gls{srl} systems can reason about any aspect of the domain.
We believe this open many doors for relational representation learning, where one interesting avenue might be to learn representations for faster inference and tackling the challenge of \textit{lifted inference}~\cite{VdBThesis13}.










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Keep the following \cleardoublepage at the end of this file,
% otherwise \includeonly includes empty pages.
\cleardoublepage

% vim: tw=70 nocindent expandtab foldmethod=marker foldmarker={{{}{,}{}}}
